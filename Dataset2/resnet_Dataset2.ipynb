{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_X58bgX-EEl0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the dataset class\n",
        "# Function to list all image files in a directory\n",
        "def list_files(dir):\n",
        "    file_list = []\n",
        "    for root, dirs, files in os.walk(dir):\n",
        "        for file in files:\n",
        "            if file.endswith(\".png\"):\n",
        "                file_list.append(os.path.join(root, file))\n",
        "    return file_list"
      ],
      "metadata": {
        "id": "3m7ubcGbEMyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory containing folders where each folder represents a class or label\n",
        "data_dir = '/content/drive/MyDrive/DATA'\n",
        "\n",
        "# Directory  to save the split data\n",
        "output_dir = '/content/drive/MyDrive/DATA'\n",
        "\n",
        "# Percentage of data to use for training and validation (rest will be used for testing)\n",
        "train_val_split = 0.8\n",
        "val_split = 0.1\n",
        "\n",
        "# Iterate through the folders (classes) in the data directory\n",
        "for class_dir in os.listdir(data_dir):\n",
        "    # Construct the full path to the class directory\n",
        "    class_path = os.path.join(data_dir, class_dir)\n",
        "\n",
        "    # Skip if class_path is not a directory\n",
        "    if not os.path.isdir(class_path):\n",
        "        continue\n",
        "\n",
        "    print(f\"Processing {class_dir} class...\")\n",
        "\n",
        "    # List all the PNG files in the class directory (including subdirectories)\n",
        "    files = list_files(class_path)\n",
        "\n",
        "    # Shuffle the files randomly\n",
        "    random.shuffle(files)\n",
        "\n",
        "    # Calculate the number of files for training, validation, and testing\n",
        "    num_train_val_files = int(train_val_split * len(files))\n",
        "    num_val_files = int(val_split * num_train_val_files)\n",
        "\n",
        "    train_val_files = files[:num_train_val_files]\n",
        "    train_files = train_val_files[:-num_val_files]\n",
        "    val_files = train_val_files[-num_val_files:]\n",
        "    test_files = files[num_train_val_files:]\n",
        "\n",
        "    # Create corresponding directories in the output directory\n",
        "    train_class_dir = os.path.join(output_dir, 'train', class_dir)\n",
        "    val_class_dir = os.path.join(output_dir, 'val', class_dir)\n",
        "    test_class_dir = os.path.join(output_dir, 'test', class_dir)\n",
        "    os.makedirs(train_class_dir, exist_ok=True)\n",
        "    os.makedirs(val_class_dir, exist_ok=True)\n",
        "    os.makedirs(test_class_dir, exist_ok=True)\n",
        "\n",
        "    # Copy training files to the train directory\n",
        "    for file_path in train_files:\n",
        "        file_name = os.path.basename(file_path)\n",
        "        dst = os.path.join(train_class_dir, file_name)\n",
        "        shutil.copy(file_path, dst)\n",
        "\n",
        "    # Copy validation files to the validation directory\n",
        "    for file_path in val_files:\n",
        "        file_name = os.path.basename(file_path)\n",
        "        dst = os.path.join(val_class_dir, file_name)\n",
        "        shutil.copy(file_path, dst)\n",
        "\n",
        "    # Copy testing files to the test directory\n",
        "    for file_path in test_files:\n",
        "        file_name = os.path.basename(file_path)\n",
        "        dst = os.path.join(test_class_dir, file_name)\n",
        "        shutil.copy(file_path, dst)\n",
        "\n",
        "print(\"Data split completed successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U56AaVrRETCS",
        "outputId": "4beda2e2-fac5-4fe6-ebcb-37b7d4a94b4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 54 class...\n",
            "Processing 5 class...\n",
            "Processing 3 class...\n",
            "Processing 55 class...\n",
            "Processing 56 class...\n",
            "Processing 35 class...\n",
            "Processing 57 class...\n",
            "Processing 7 class...\n",
            "Processing 4 class...\n",
            "Processing 30 class...\n",
            "Processing 0 class...\n",
            "Processing 26 class...\n",
            "Processing 16 class...\n",
            "Processing 11 class...\n",
            "Processing 24 class...\n",
            "Processing 10 class...\n",
            "Processing 28 class...\n",
            "Processing 17 class...\n",
            "Processing 14 class...\n",
            "Processing 12 class...\n",
            "Data split completed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_root = '/content/drive/MyDrive/DATA'\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])"
      ],
      "metadata": {
        "id": "EP2wKKXqEikT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RoadSignDataset(Dataset):\n",
        "    def __init__(self, root_dir, split=\"train\", transform=None):\n",
        "        self.root_dir = os.path.join(root_dir, split)\n",
        "        self.transform = transform\n",
        "\n",
        "        # Get the list of class folders (labels)\n",
        "        self.class_names = sorted(os.listdir(self.root_dir))\n",
        "\n",
        "        # Initialize lists to store image paths and corresponding labels\n",
        "        self.image_paths = []\n",
        "        self.labels = []\n",
        "\n",
        "        # Iterate over class folders\n",
        "        for label_idx, class_name in enumerate(self.class_names):\n",
        "            class_dir = os.path.join(self.root_dir, class_name)\n",
        "\n",
        "            # Get list of image files in the class folder\n",
        "            image_files = [f for f in os.listdir(class_dir) if f.endswith('.png')]\n",
        "\n",
        "            # Append image paths and corresponding labels\n",
        "            self.image_paths.extend([os.path.join(class_dir, img) for img in image_files])\n",
        "            self.labels.extend([label_idx] * len(image_files))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "c1e8nHC_ElNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define dataset and data loaders for train and validation sets\n",
        "train_dataset = RoadSignDataset(root_dir=dataset_root, split='train', transform=transform)\n",
        "test_dataset = RoadSignDataset(root_dir=dataset_root, split='test', transform=transform)\n",
        "val_dataset = RoadSignDataset(root_dir=dataset_root, split='val', transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
      ],
      "metadata": {
        "id": "BGS9M6-XEqrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the ResNet model\n",
        "model = models.resnet18(pretrained=False)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, len(train_dataset.class_names))\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Move model to device if available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTTKuEhfEuSz",
        "outputId": "68eca3a8-b35d-4610-9567-cd2d2bb259b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=20, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training and evaluation\n",
        "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=25):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        running_accuracy = 0.0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            running_accuracy += (predicted == labels).float().mean().item()\n",
        "\n",
        "        train_loss = running_loss / len(train_loader)\n",
        "        train_accuracy = running_accuracy / len(train_loader)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_running_loss = 0.0\n",
        "        val_running_accuracy = 0.0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                val_loss = criterion(outputs, labels)\n",
        "                val_running_loss += val_loss.item()\n",
        "\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                val_running_accuracy += (predicted == labels).float().mean().item()\n",
        "\n",
        "        val_loss = val_running_loss / len(val_loader)\n",
        "        val_accuracy = val_running_accuracy / len(val_loader)\n",
        "\n",
        "        print(f'Epoch {epoch + 1}/{num_epochs}, '\n",
        "              f'Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.2%}, '\n",
        "              f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2%}')\n",
        "\n",
        "    print('Training completed')"
      ],
      "metadata": {
        "id": "yE2DNY4GEyS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "train_model(model, criterion, optimizer, train_loader, val_loader)\n",
        "\n",
        "# Evaluate the model on test set\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f'Test Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Evaluate the model\n",
        "evaluate_model(model, test_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sc9CE4M_E2jd",
        "outputId": "8543beb8-e266-47de-cc79-41182da91a6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25, Training Loss: 2.5684, Training Accuracy: 23.81%, Validation Loss: 2.2864, Validation Accuracy: 31.25%\n",
            "Epoch 2/25, Training Loss: 1.8294, Training Accuracy: 44.54%, Validation Loss: 1.7290, Validation Accuracy: 46.09%\n",
            "Epoch 3/25, Training Loss: 1.4295, Training Accuracy: 56.16%, Validation Loss: 1.5546, Validation Accuracy: 49.36%\n",
            "Epoch 4/25, Training Loss: 1.1975, Training Accuracy: 62.02%, Validation Loss: 1.3619, Validation Accuracy: 57.96%\n",
            "Epoch 5/25, Training Loss: 1.0431, Training Accuracy: 67.65%, Validation Loss: 1.2177, Validation Accuracy: 62.13%\n",
            "Epoch 6/25, Training Loss: 0.8987, Training Accuracy: 73.11%, Validation Loss: 1.1664, Validation Accuracy: 63.45%\n",
            "Epoch 7/25, Training Loss: 0.7960, Training Accuracy: 75.13%, Validation Loss: 1.0630, Validation Accuracy: 68.94%\n",
            "Epoch 8/25, Training Loss: 0.6927, Training Accuracy: 81.82%, Validation Loss: 0.9870, Validation Accuracy: 75.19%\n",
            "Epoch 9/25, Training Loss: 0.5709, Training Accuracy: 85.74%, Validation Loss: 1.0267, Validation Accuracy: 80.02%\n",
            "Epoch 10/25, Training Loss: 0.4628, Training Accuracy: 89.35%, Validation Loss: 0.7599, Validation Accuracy: 80.65%\n",
            "Epoch 11/25, Training Loss: 0.4158, Training Accuracy: 90.62%, Validation Loss: 0.9825, Validation Accuracy: 77.92%\n",
            "Epoch 12/25, Training Loss: 0.3316, Training Accuracy: 92.61%, Validation Loss: 0.7402, Validation Accuracy: 83.26%\n",
            "Epoch 13/25, Training Loss: 0.3048, Training Accuracy: 93.75%, Validation Loss: 0.8027, Validation Accuracy: 86.90%\n",
            "Epoch 14/25, Training Loss: 0.2381, Training Accuracy: 94.41%, Validation Loss: 0.6157, Validation Accuracy: 89.00%\n",
            "Epoch 15/25, Training Loss: 0.2045, Training Accuracy: 96.26%, Validation Loss: 0.4437, Validation Accuracy: 92.66%\n",
            "Epoch 16/25, Training Loss: 0.1545, Training Accuracy: 96.83%, Validation Loss: 0.8686, Validation Accuracy: 83.26%\n",
            "Epoch 17/25, Training Loss: 0.1347, Training Accuracy: 97.36%, Validation Loss: 0.4091, Validation Accuracy: 93.44%\n",
            "Epoch 18/25, Training Loss: 0.1314, Training Accuracy: 97.76%, Validation Loss: 0.4562, Validation Accuracy: 92.93%\n",
            "Epoch 19/25, Training Loss: 0.1145, Training Accuracy: 97.93%, Validation Loss: 0.4342, Validation Accuracy: 92.78%\n",
            "Epoch 20/25, Training Loss: 0.1112, Training Accuracy: 97.84%, Validation Loss: 0.5589, Validation Accuracy: 91.08%\n",
            "Epoch 21/25, Training Loss: 0.0945, Training Accuracy: 97.93%, Validation Loss: 0.4469, Validation Accuracy: 93.44%\n",
            "Epoch 22/25, Training Loss: 0.0935, Training Accuracy: 98.02%, Validation Loss: 0.4235, Validation Accuracy: 94.88%\n",
            "Epoch 23/25, Training Loss: 0.0955, Training Accuracy: 98.06%, Validation Loss: 0.4039, Validation Accuracy: 92.66%\n",
            "Epoch 24/25, Training Loss: 0.0737, Training Accuracy: 98.15%, Validation Loss: 0.4072, Validation Accuracy: 95.00%\n",
            "Epoch 25/25, Training Loss: 0.1025, Training Accuracy: 97.98%, Validation Loss: 0.3310, Validation Accuracy: 95.15%\n",
            "Training completed\n",
            "Test Accuracy: 97.78%\n"
          ]
        }
      ]
    }
  ]
}